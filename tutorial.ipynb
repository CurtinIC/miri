{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fireball detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks through how to develop a workflow for generating synthetic data and training a machine learning model to detect meteors from images.\n",
    "\n",
    "This tutorial has been prepared by:\n",
    "- [Andrew Rohl](http://computation.curtin.edu.au/about/steering-committee/director/)\n",
    "- [Shiv Meka](http://computation.curtin.edu.au/about/computational-specialists/humanities/)\n",
    "- [Kevin Chai](http://computation.curtin.edu.au/about/computational-specialists/health-sciences/)\n",
    "\n",
    "from the [Curtin Institute for Computation](http://computation.curtin.edu.au) at Curtin University in Perth, Australia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Curtin Institute for Computation was asked to develop a machine learning model to detect meteor trails (fireballs) by researchers from the [Curtin Institute for Radio Astronomy](http://astronomy.curtin.edu.au/) ([ICRAR](https://www.icrar.org/) - Curtin University node) from images of the night sky captured in the Australian desert by the [Desert Fireball Network](http://fireballsinthesky.com.au/). \n",
    "\n",
    "![Fireballs](images/desert_fireball_network.png)\n",
    "\n",
    "<div style=\"text-align:center;font-weight:bold\">Figure: Desert Fireball Network camera locations</div>\n",
    "\n",
    "The objective of the project is to find these meteors in optical images and to compare against the radio emissions recorded at the same time and location by the [Murchison Widefield Array](http://www.mwatelescope.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Curtin Institute for Radio Astronomy (CIRA)*\n",
    "* Paul Hancock\n",
    "* Xiang Zhang\n",
    "* Sean Mattingley\n",
    "* Steven Tingay\n",
    "* Randall Wayth\n",
    "\n",
    "*Desert Fireball Network*\n",
    "* Hadrien Devillepoix\n",
    "* Phil Bland\n",
    "\n",
    "*Curtin Institute for Computation*\n",
    "* Shiv Meka\n",
    "* Kevin Chai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "\n",
    "The dataset is comprised of images taken during the Geminid meteor shower on the 14th of December 2015. Images were captured from a camera based at a station and a camera that was set up on the back of a truck (mobile) to capture images from multiple locations. The images were reviewed and annotated by astronomers to construct the dataset shown in Table 1.\n",
    "\n",
    "<p style=\"text-align:center;font-weight:bold\">Table 1: Dataset</p>\n",
    "\n",
    "| Camera      | Images    | Meteors\n",
    "|:------------|:----------|:--------|\n",
    "| mobile      | 1,561     | 48      |\n",
    "| station     | 1,330     | 24      |\n",
    "\n",
    "The raw dataset contains RGB images with a resolution of 7360x4912 pixels and are ~6.3MB each. Meteors within these images were manually identified by astronomers within the project team. Examples of these meteors are shown below.\n",
    "\n",
    "![Fireballs](images/meteors.png)\n",
    "\n",
    "<div style=\"text-align:center;font-weight:bold\">Figure: Cropped images containing meteors in the dataset</div>\n",
    "\n",
    "From preliminary experiments, we observed that meteors could be identified without colour and using lower resolution images. The benefit of compressing the images reduces the computational overhead required and therefore allows us to train a meteor detection model faster. Transforming the raw images into grayscale and to a resolution of 1840x1228 resulted in ~238KB for each image. An example is shown below\n",
    "\n",
    "![Fireballs](images/night_sky_example.jpg)\n",
    "\n",
    "<div style=\"text-align:center;font-weight:bold\">Figure: Transformed night sky image example</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetic data\n",
    "The dataset does not contain enough meteors to develop a robust and accurate machine learning model. Therefore, we have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py \n",
    "from PIL import Image,ImageDraw\n",
    "import numpy as np\n",
    "\n",
    "def fake_image(file_name=\"img.h5\",batch_size=80):\n",
    "    '''\n",
    "    file_name: hdf file with images of night_sky and validation\n",
    "    batch_size: images per step\n",
    "    '''\n",
    "    filter_w=200 #output image width\n",
    "    filter_h=200 #output image height\n",
    "    np.random.seed(0)\n",
    "    def dist(A): #Computes distance between two points in px\n",
    "        return(((A[0][0]-A[0][2])**2+(A[0][1]-A[0][3])**2)**0.5)\n",
    "\n",
    "    count=0\n",
    "    hdf=h5py.File(\"img.h5\",\"r\")\n",
    "    bg_imgs=hdf[\"night_sky\"][:]\n",
    "    out_images=[] #list to store\n",
    "    out_preds=[]\n",
    "    while True:\n",
    "        for i in range(batch_size):\n",
    "            myrand=np.random.randint(bg_imgs.shape[0]) #Open files with random background and w/o meteorites\n",
    "            im=Image.fromarray(bg_imgs[myrand]) #im stores images with random background\n",
    "            width,height=im.size\n",
    "            marker_x=np.random.randint(0,width-filter_w)\n",
    "            marker_y=np.random.randint(0,width-filter_h)\n",
    "            #crop a portion of the image\n",
    "            im=im.crop((marker_x,marker_y,marker_x+filter_w,marker_y+filter_h))\n",
    "            draw = ImageDraw.Draw(im)\n",
    "            A=[(np.random.rand()*im.size[0], np.random.rand()*im.size[1],\n",
    "                np.random.rand()*im.size[0], np.random.rand()*im.size[1])]\n",
    "            myrand=str(myrand)\n",
    "            flag_present=False\n",
    "            if (np.random.rand()>0.5): #Statistically, only half the samples would have meteorites\n",
    "                if ((dist(A)>30) and (dist(A)<300)): #The lines shouldn't be too short or too long\n",
    "                    color_rnd=np.random.uniform(0.8,1.0)  #Brightness should be over certain threshold, lower the brightness -> harder to train, more resilient\n",
    "                    width_rand=np.random.choice([1,2])\n",
    "                    draw.line([A[0][0],A[0][1],A[0][2],A[0][3]], fill=int(color_rnd*250),width=width_rand)\n",
    "                    flag_present=True\n",
    "                    del draw\n",
    "            out_images.append(np.array(im).reshape([filter_w,filter_h,1]))\n",
    "            out_preds.append(flag_present*1)\n",
    "        yield np.array(out_images),np.array(out_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning\n",
    "The dataset does not contain enough meteors to develop a robust and accurate machine learning model. Therefore, we have\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input,Conv2D,MaxPool2D,Dense,BatchNormalization,Flatten\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "#Basic hello world CNN\n",
    "inp=Input(shape=(200,200,1))\n",
    "Activation='relu'\n",
    "out=[]\n",
    "#3 layers of CNN-MaxPool cascades\n",
    "for i in range(3):\n",
    "    out=Conv2D(8+(i*2),3,activation=Activation)((out,inp)[i==0])\n",
    "    out=MaxPool2D(2)(out)\n",
    "    out=BatchNormalization()(out)\n",
    "out=Flatten()(out)\n",
    "#FF dense stack\n",
    "for i in range(5):\n",
    "    out=Dense(5)(out)\n",
    "out=Dense(1,activation='sigmoid')(out)\n",
    "mod=Model(inp,out)\n",
    "mod.compile(optimizer='adagrad',loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.fit_generator(fake_image(),steps_per_epoch=10,epochs=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
