{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples and code for explaining machine learning and deep learning concepts.\n",
    "\n",
    "This tutorial has been prepared by:\n",
    "- [Andrew Rohl](http://computation.curtin.edu.au/about/steering-committee/director/)\n",
    "- [Shiv Meka](http://computation.curtin.edu.au/about/computational-specialists/humanities/)\n",
    "- [Kevin Chai](http://computation.curtin.edu.au/about/computational-specialists/health-sciences/)\n",
    "\n",
    "from the [Curtin Institute for Computation](http://computation.curtin.edu.au) at Curtin University in Perth, Australia for the [7th International Conference on Smart Computing & Communications (ICSCC 2019)](http://icscc.online/) hosted at Curtin University in Miri, Sarawak, Malaysia on the 28-30 June 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Installation\n",
    "\n",
    "Install the necessary python dependencies. Some packages marked with an __^__ are available on <a href='https://www.anaconda.com/distribution/' target='_blank'>Anaconda's Python </a>distribution by default and for the others you can use a package installer - `pip` or `easy_install`: \n",
    "\n",
    "- Python 3 ^\n",
    "- jupyter ^\n",
    "- numpy ^\n",
    "- scipy ^\n",
    "- PIL ^\n",
    "- tarfile ^\n",
    "- requests ^\n",
    "- tensorflow\n",
    "- keras\n",
    "- plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check installed packages\n",
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries needed for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all essential subroutines for data processing and visualization\n",
    "from workshop import * \n",
    "\n",
    "# Plotting libraries\n",
    "import plotly.graph_objs as go \n",
    "from plotly.offline import plot, iplot, init_notebook_mode \n",
    "import matplotlib.pyplot as PyPlot\n",
    "\n",
    "# Scientific library needed for most exercises\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning functions\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# Plot graphs in the notebook itself w/o opening a new window\n",
    "init_notebook_mode(True)\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Curve fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display slides for the section\n",
    "PDF('assets/exercise1.pdf', 3, 600, 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Measure distance / loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Mean-squared distance\n",
    "def mse(inp, pred):\n",
    "    '''\n",
    "    Mean-squared error (MSE) amplifies the loss between predicted value and ground truth.\n",
    "    It just is squared difference between the two.\n",
    "    This 'amplification' helps in faster convergence in regression problems as compared\n",
    "    to just difference between the values - also called Mean Average error.\n",
    "    '''\n",
    "    return np.sum(np.square(inp - pred)) #inp: Inputs and pred: predictions from your test model\n",
    "\n",
    "# Computes error between actual and predicted values\n",
    "def distance(a, X, pred, values):\n",
    "    '''\n",
    "    MSE (look at the function above) quantifies loss between ground truth and predicted values. Now, these\n",
    "    predictions are based on something called a hypothesis - a function we use\n",
    "    to hypothesize / estimate the trend. Visualize the data first before \n",
    "    engineering a hypothesis function.\n",
    "    \n",
    "    '''\n",
    "    inp = np.exp(a[0] * x) * np.sin(a[1] * x) #<-----Tweak the hypothesis here\n",
    "    msd = mse(inp, pred) #Error between prediction and hypothesis for values a,b\n",
    "    values.append([inp]) \n",
    "    print (\"Variables (a, b):\", a ,\"Loss:\", msd) #Iterates over a,b to find the best loss\n",
    "    return msd\n",
    "  \n",
    "def visualize(optim_step):\n",
    "    '''\n",
    "    Function that is used to plot the goodness of fit for each optimization timestep.\n",
    "    '''\n",
    "    import matplotlib.pyplot as pyp\n",
    "    global x, y, values\n",
    "    pyp.plot(x, y, x, np.array(values[optim_step]).T[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Trump dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "solution2": "shown",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "global x, y # Accessible across the notebook\n",
    "data = np.load('assets/trump.npy')\n",
    "\n",
    "# y = f(x)\n",
    "x = data.item(0)['x']\n",
    "y = data.item(0)['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by visualising the dataset. a.k.a. data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iplot({\"data\": [go.Scatter(x=x, y=y)]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you think of a `hypothesis` i.e a mathematical function to approximate Trump's mood swings?\n",
    "\n",
    "Let's use a traditional optimiser to learn a good function to fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as opt\n",
    "\n",
    "# A placeholder to log loss vs. time\n",
    "global values\n",
    "values = [] \n",
    "\n",
    "# To change the hypothesis, have a look at the function - 'distance'. \n",
    "# Minimizes the distance between prediction and expected using conjugate gradient\n",
    "opt.minimize(distance, (0.0, 0.0), (x, y, values), method='cg') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Plot the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize optimization steps\n",
    "# This only works on a laptop - Google blocks ipywidgets \n",
    "_ = interact(visualize,optim_step = widgets.IntSlider(min=0, max=max(0, len(values) - 1), step=10, continuous_update=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4: Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the previous example, we cheated (in a way) as we knew the functional form of the curve $(sin(a_{0}x) \\cdot e^{-a_{1}x}))$, and ran a parametric optimisation. \n",
    "\n",
    "This section demonstrates a 'magical' hypothesis function that could be used to predict almost anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF('assets/exercise2.pdf', 1, 600, 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "### 4.1 Neural networks\n",
    "\n",
    "Demostrate logistic regression, feature engineering, shallow and deep neural networks on the [TensorFlow playground](https://playground.tensorflow.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Fitting to random data \n",
    "Generate a random dataset with two input features: mood swings and blood pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate two random features, and a random output. Is there a model that could fit?\n",
    "np.random.seed(0)\n",
    "#Generate two random features, and a random output. Is there a model that could fit?\n",
    "random_train_input,random_train_output=generate_random(40,400,2)\n",
    "random_test_input,random_test_output=generate_random(10,400,2)\n",
    "\n",
    "# Plotting the input features as well as output\n",
    "iplot([{\"x\":np.arange(len(random_train_input[0])), \"y\":random_train_input[0,:,0],\"name\":\"input1\"},\n",
    "      {\"x\":np.arange(len(random_train_input[0])), \"y\":random_train_input[0,:,1],\"name\":\"input2\"},\n",
    "      {\"x\":np.arange(len(random_train_input[0])), \"y\":random_train_output[0,:],\"name\":\"output\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a simple 4 layer neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization,Dense,Input,Flatten\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "#The function below generates a neural network model with n_layers\n",
    "def dense_model(n_layers,timesteps=400,features=2):\n",
    "    '''\n",
    "    A hello world Machine learning model\n",
    "    '''\n",
    "    np.random.seed(1)\n",
    "    inp=Input(shape=[timesteps,features])\n",
    "    out=Flatten()(inp)\n",
    "    for i in range(n_layers):\n",
    "        out=Dense((42,timesteps)[i==n_layers-1])(out)\n",
    "    return Model(inp,out)\n",
    "\n",
    "model = dense_model(4)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to learn good values for the weights and biases in our neural network. Let's try the same traditional optimisation strategy we used for the curve fitting example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run optimization using full hessian\n",
    "# Note: This should take about 2 mins to complete\n",
    "# You can interrupt the notebook kernel if it takes too long\n",
    "viz_loss = []\n",
    "%time opt.minimize(optimize_cg, convert_weights(model, model.get_weights()),(model, random_train_input, random_train_output, viz_loss), method='cg', options={'maxiter': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions and ground truth values\n",
    "# Note: zoom in the plot see the differences\n",
    "iplot([{\"x\":np.arange(len(random_train_input[0])),\"y\":model.predict(random_train_input)[0],\"name\":'predictions'},\n",
    "      {\"x\":np.arange(len(random_train_input[0])), \"y\":random_train_output[0,:],\"name\":'ground truth'}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the optimiser was unable to learn good weights and bias values for the neural network to accurately predict the ground truth. This highlights the limitations of using traditional optimisation methods for training neural networks. \n",
    "\n",
    "It should be noted that this example model is very small only comprising of 361 parameters. In practice, it is not uncommon to train neural networks with 100s of thousands to 10s of millions of parameters for complex tasks such as object detection and image segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Backpropagation\n",
    "Let's re-train the model using a different optimisation strategy called <b>backpropagation</b> (backprop for short). Backprop has the advantage of optimising (updating) values for a given neural network layer with only its connected neighbours / layers. \n",
    "\n",
    "Technically speaking, this causes the hessian to be sparse as parameters are decoupled from other non-neighbouring parameters in the graph which results in faster optimisation.\n",
    "\n",
    "To illustrate, we can define our neural network as:\n",
    "\n",
    "$network=f_{1}(\\ f_{2} (\\ f_{3}(\\ f_{4}(x)))))$\n",
    "\n",
    "where $x$ represents our input data and the function $f_{n}$ represents the _nth_ layer in the network. For example, parameters in the second layer $f_{2}$ only interact with parameters $f_{1}$ and $f_{3}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# Reinitalise the model (i.e. reset weights)\n",
    "model = dense_model(2)\n",
    "# Use the adam (ADAptive Momentum) optimisation algorithm \n",
    "# with the mse (mean square error) loss function\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for 100 epochs (number of training iterations through the whole dataset)\n",
    "# Note: This should take about 1min to complete training\n",
    "model.fit(random_train_input, random_train_output, \n",
    "                validation_data=(random_test_input,random_test_output),\n",
    "                epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "solution2": "shown",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# Plot predictions and ground truth values\n",
    "# Note: zoom in the plot see the differences\n",
    "iplot([{\"x\":np.arange(400),\"y\":random_train_output[0,:],\"name\":\"input\"},\n",
    "      {\"x\":np.arange(400),\"y\":model.predict(random_train_input)[0],\"name\":\"predicted\"}])\n",
    "\n",
    "#replace train with test, what do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many backprop optimisation algorithms we can use for training neural networks. Each type has advantages and disadvantages in terms of speed and stability for a given dataset and problem. Some most popular algorithms used for training neural networks are shown below.\n",
    "\n",
    "<img src='assets/optimizers.gif' width='50%'>\n",
    "<center>Source: Sebastian Ruder (2018) <a href='http://rudio.io' target='_blank'>http://rudio.io</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained a simple vanilla neural network (a multi-layer perception - MLP) in the previous section. However, there are other types of neural networks which are better suited for specific types of data and problems. For example, convolutional neural networks (CNNs) are a popular type of network used for image and signal data.\n",
    "\n",
    "A CNN is comprised of convolution layers. A convolution is a mathematical operation where some input data (e.g. image) is transformed by a convolution matrix of numbers (filter) to generate some output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PDF(\"assets/exercise3.pdf\", 1, 600, 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helper functions used for interactive visualisations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     39
    ]
   },
   "outputs": [],
   "source": [
    "# Ignore this function - this is only used to visualize\n",
    "from PIL import Image # Library to load images\n",
    "def Infer(Num_filters, Num_layers, max_pool=False):\n",
    "    global X, parameters, max_pooling\n",
    "    K.clear_session()\n",
    "    inp = Input(shape=(788, 1180, 3))\n",
    "    out = []\n",
    "    N = Num_layers\n",
    "    parameters = []\n",
    "    max_pooling = []\n",
    "    # Conv-Maxpool - fully connected deck\n",
    "    for i in range(N):\n",
    "        out = Conv2D(Num_filters, (3, 3), activation='relu')((out, inp)[i==0])\n",
    "        if max_pool:\n",
    "            out = MaxPool2D(2)(out)\n",
    "    out = Flatten()(out)\n",
    "    out = Dense(1, activation='sigmoid')(out)\n",
    "\n",
    "    model = Model(inp,out)\n",
    "    \n",
    "    #how many trainable parameters?\n",
    "    params = int(np.sum([K.count_params(i) for i in set(model.trainable_weights)]))\n",
    "\n",
    "    #For plotting\n",
    "    parameters.append(params)\n",
    "    max_pooling.append(int(max_pool))\n",
    "    print (\"Parameters:\", params)\n",
    "    \n",
    "    plot_model(model, to_file='assets/model.png')\n",
    "\n",
    "    return HTML('<img src=\"assets/model.png?' + str(np.random.randint(0, 1E8)) +' height=\"240\" width=\"200\">')\n",
    "\n",
    "def Generate_maps(kernel_size=3, stride=1):\n",
    "    K.clear_session()\n",
    "    image='assets/lake.jpg'\n",
    "    img=np.expand_dims(np.array(PIL.Image.open(image)), 0)\n",
    "    inp=Input(shape=(None, None, 3))\n",
    "    out=Conv2D(4, int(kernel_size), strides=int(stride), padding='same')(inp)\n",
    "    out=Conv2D(1, int(kernel_size), strides=int(stride), padding='same')(out)\n",
    "    model=Model(inp, out)\n",
    "\n",
    "    return display(Image.fromarray(np.uint8(model.predict(img))[0, :, :, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Convolutional neural network\n",
    "To illustrate convolutions, we define a simple two layer CNN that takes a RGB image (3-channels) and outputs a 1-channel image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D\n",
    "import keras.backend as K\n",
    "import PIL\n",
    "\n",
    "np.random.seed(17)\n",
    "inp = Input(shape=(None, None, 3))\n",
    "# conv layer 1: 4 filter, 3 x 3 matrix, default stride of 1\n",
    "out = Conv2D(4, 3, padding='same')(inp)\n",
    "# conv layer 2: 1 filter, 3 x 3 matrix, default stride of 1\n",
    "out = Conv2D(1, 3, padding='same')(out)\n",
    "model = Model(inp, out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show an example image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open('assets/lake.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the image through our CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You may ignore the reshaping procedure as PIL.Image expects just 2 dimensions [X,Y] pixels to print the image\n",
    "img = np.expand_dims(np.array(Image.open('assets/lake.jpg')), 0) \n",
    "Image.fromarray(np.uint8(model.predict(img)[0, :, :, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the output of passing the original image through the convolution layers in our network. i.e. The original 3-channel (RGB) input has now been transformed into a 1-channel (single colour) output image.\n",
    "\n",
    "Note: we have not trained our model on any data but rather used the convolution matrix values that were randomly generated when initialising the model. Surprisingly, these random convolution matrices (filters) were able to do a pretty good job of highlighting (segmenting) the rocks on the left hand side of the image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Filters and strides\n",
    "We can change the number of filters and stride size in each convolution layer. Let's experiment with different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "button=widgets.interact_manual(Generate_maps,\n",
    "                               kernel_size=widgets.IntSlider(min=2, max=10, step=1, continuous_update=False),\n",
    "                               stride=widgets.IntSlider(min=1, max=10, step=1, continuous_update=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- increase kernel size = blurier output\n",
    "- increase stride size = smaller output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Network output\n",
    "So far we designed a CNN that takes in an image and outputs a transformed image. \n",
    "\n",
    "However, for many problems we want our model to output numbers or a category value rather than a transformed image. For example, if developed a CNN to identify tumours in MRI/CT images, we would want our model to output a probability or category (i.e. tumour / no tumour) for a given image. \n",
    "\n",
    "These are referred to as classification problems and an example CNN used for image classification is depicted below.\n",
    "\n",
    "<img src='assets/cnn.png'>\n",
    "<center>Example of a CNN to classify images</center>\n",
    "\n",
    "Let's update our model to follow this example CNN architecture ignoring the pooling layers for the moment. \n",
    "\n",
    "We will make our model to output the probability of the image belonging to a category by appending two additional layers: `Flatten` and `Dense`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, Flatten, Dense\n",
    "from keras.utils import plot_model\n",
    "\n",
    "np.random.seed(17)\n",
    "inp = Input(shape=(img.shape[1], img.shape[2], 3))\n",
    "# conv layer 1: 4 filter, 3 x 3 matrix, default stride of 1\n",
    "out = Conv2D(4, 3, padding='same')(inp)\n",
    "# conv layer 2: 1 filter, 3 x 3 matrix, default stride of 1\n",
    "out = Conv2D(1, 3, padding='same')(out)\n",
    "# Flatten the conv layer 2 into a vector of values\n",
    "# This is required in order to connect the convolution layer with the fully connected layer\n",
    "out = Flatten()(out)\n",
    "# Add a fully connected layer with 1 output\n",
    "# The sigmoid activation function generates a probability score (value between 0 and 1)\n",
    "out = Dense(1, activation='sigmoid')(out)\n",
    "model = Model(inp, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the image through our classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Prediction')\n",
    "print('  probability: %.5f' % (model.predict(img)[0][0]))\n",
    "print('  category: %d' % (round(model.predict(img)[0][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted probability of the image belonging to category 1 is 0.00006 (0.006%). As such, we classify the image as belong to the other class (category 0).\n",
    "\n",
    "Let's inspect our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of parameters has grown significantly from 149 to 929,990! You can see from the model summary that the Flatten layer is where the majority of new parameters were added.\n",
    "\n",
    "A `Flatten` layer is required to connect our convolution layers with our fully connected layer that predicts the image category. There are two main issues with designing CNNs with fully connected layers:\n",
    "\n",
    "1. It can significantly increase the number of parameters in the model \n",
    "2. The input data (image) size becomes fixed (e.g. 788 x 1180 in our example)\n",
    "\n",
    "Note: there are newer variants of CNNs known as Fully Convolutional Networks (FCNs) that replace fully connected layers with convolution layers while still being able to generate classification outputs. FCNs are able to resolve the issues mentioned above at the cost of requiring more compute for running more convolution operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling layers can be used to reduce the number of parameters in our CNN model by downsampling the outputs of the convolution layers. Max pooling is used as a pooling method in many CNNs is depicted below.\n",
    "\n",
    "<img src='assets/pooling.png' width='80%'>\n",
    "<center>Source: MXNet Tutorial, <a href='http://mxnet.io/tutorials/python/mnist.html' target='_blank'>http://mxnet.io/tutorials/python/mnist.html</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add pooling layers after each convolution layer in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, Flatten, Dense, MaxPool2D\n",
    "from keras.utils import plot_model\n",
    "\n",
    "np.random.seed(17)\n",
    "inp = Input(shape=(img.shape[1], img.shape[2], 3))\n",
    "# conv layer 1: 4 filter, 3 x 3 matrix, default stride of 1\n",
    "out = Conv2D(4, 3, padding='same')(inp)\n",
    "out = MaxPool2D()(out)\n",
    "# conv layer 2: 1 filter, 3 x 3 matrix, default stride of 1\n",
    "out = Conv2D(1, 3, padding='same')(out)\n",
    "out = MaxPool2D()(out)\n",
    "# Flatten the conv layer 2 into a vector of values\n",
    "out = Flatten()(out)\n",
    "# Add a fully connected layer with 1 output\n",
    "# The sigmoid activation function generates a probability score (value between 0 and 1)\n",
    "out = Dense(1, activation='sigmoid')(out)\n",
    "model = Model(inp, out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of pooling layers has signficantly reduced the number of parameters from 929,990 to 58,265 in our classification model.\n",
    "\n",
    "Experiment the number of filters, layers and use of max pool below to see how the number of parameters change for a given model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_=widgets.interact(Infer, Num_filters=widgets.IntSlider(min=2, max=35, step=1, continuous_update=False),\n",
    "                   Num_layers=widgets.IntSlider(min=1, max=8, step=1, continuous_update=False),\n",
    "                   max_pool=[('True', True), ('False', False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "No max pool:\n",
    "- more filters = more parameters\n",
    "- more layers = slightly less parameters (more convolutions to pass)\n",
    "\n",
    "Max pool:\n",
    "- more filters = more parameters\n",
    "- more layers = significantly less parameters (more convolutions and pooling to pass)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
